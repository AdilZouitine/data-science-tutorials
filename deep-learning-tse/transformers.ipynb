{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The goal of this class is pretty simple: Learn how work the transformers architecture from scratch and afterward train a fine tune transformer for text classification.\n",
    "Indeed if you understand well the architecture of the transformer, you will be able to use it for any task and specially for the OVERHYPED Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism in machine learning, particularly in the field of natural language processing, that allows a model to weigh the importance of different parts of an input sequence when processing it. It's a key component of Transformer models, which have revolutionized language processing, Computer vision, Speech recognition etc ... tasks. Here's a detailed explanation of how self-attention works, including the mathematical formulas involved:\n",
    "\n",
    "### 1. Input Representation\n",
    "- **Input**: Assume we have an input sequence  Self-attention is the pairwise interdependence of all elements composing an input. $$X = [x_1, x_2, ..., x_n] $$, where each $ x_i $ is a vector representing a word or token in the sequence.\n",
    "- **Embeddings**: These vectors are typically embeddings, which are dense representations of the words.\n",
    "\n",
    "### 2. Calculating Query, Key, and Value Vectors\n",
    "For each element in the sequence, the self-attention mechanism generates three vectors: a query vector $ Q $, a key vector $ K $, and a value vector $V$. These are computed as follows:\n",
    "\n",
    "$$ Q = XW^Q $$\n",
    "$$ K = XW^K $$\n",
    "$$ V = XW^V $$\n",
    "\n",
    "Where $ W^Q $, $ W^K $, and $ W^V $ are weight matrices that are learned during training.\n",
    "\n",
    "### 3. Attention Score Calculation\n",
    "The attention scores are calculated using the query and key vectors. For each pair of words in the sequence, the score represents how much focus to put on other parts of the input when processing a specific part of the input.\n",
    "\n",
    "$$ \\text{Attention Score} = \\frac{QK^T}{\\sqrt{d_k}} $$\n",
    "\n",
    "- $d_k$ is the dimension of the key vectors. The division by $ \\sqrt{d_k} $ is for scaling purposes to avoid very large values during training.\n",
    "\n",
    "### 4. Softmax Normalization\n",
    "The attention scores are normalized using the softmax function to ensure they sum to 1 (converting them into probabilities):\n",
    "\n",
    "$$ \\text{Softmax}(\\text{Attention Score}) = \\frac{\\exp(\\text{Attention Score})}{\\sum \\exp(\\text{Attention Score})} $$\n",
    "\n",
    "### 5. Output Calculation\n",
    "Finally, the output is calculated as a weighted sum of the value vectors, using the normalized attention scores as weights:\n",
    "\n",
    "$$ \\text{Output} = \\text{Softmax}(\\text{Attention Score}) \\times V $$\n",
    "\n",
    "### 6. Multi-Head Attention\n",
    "In practice, multiple sets of $$ Q, K, V $$ matrices are used to create multiple \"heads\" of attention. This allows the model to focus on different parts of the input simultaneously. The outputs of these heads are then concatenated and linearly transformed into the final output.\n",
    "\n",
    "\n",
    "![](https://boring-guy.sh/img/masking-rl/multi_head_attention.svg)\n",
    "\n",
    "Mathematically we can translate the figure into the following equation:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V, Mask)=\\operatorname{softmax}\\left(\\frac{Mask(Q K^{T})}{\\sqrt{d_{k}}}\\right) V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { MultiHead }(Q, K, V, Mask)= \\operatorname{Concat}(\\text { head } {1}, \\ldots, \\text { head }_{h}) W^{O}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { where head }{i} = \\text { Attention }(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}, Mask)\n",
    "$$\n",
    "\n",
    "\n",
    "### 7. Role in Transformers\n",
    "In Transformer models, this self-attention mechanism is used both in the encoder to process the input sequence and in the decoder to generate the output sequence, allowing the model to consider the entire input sequence when making predictions.\n",
    "\n",
    "This explanation provides a basic overview of the self-attention mechanism. The actual implementation involves additional details and optimizations, especially in large-scale models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png)\n",
    "A transformer architecture is simply a bunch of transformer encoder layer and transformer decoder layer stacked on top of each other. The encoder is used to encode the input sequence and the decoder is used to generate the output sequence. The encoder and decoder are connected to each other using the attention mechanism.\n",
    "\n",
    "### 8. References\n",
    "- [Self-Attention in NLP](https://www.youtube.com/watch?v=5vcj8kSwBCY)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the tranformers, from scratch, you will use this following tensor as example\n",
    "dummy_tensor = torch.randn((16, 32, 64 )) # A tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "# you can imagine a batch of 16 sentence of 32 words, each word being a 64-dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.0: </b>\n",
    "* Implement the self-attention mechanism\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module): \n",
    "    def __init__(self, input_dim: int, dim_head: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.query = nn.Linear(input_dim, dim_head)\n",
    "        self.key = nn.Linear(input_dim, dim_head)\n",
    "        self.value = nn.Linear(input_dim, dim_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, sequence: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        sequence: tensor of shape (batch_size, sequence_length, input_dim)\n",
    "        attention_mask: tensor of shape (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "        # Compute Q, K, V\n",
    "        Q = \n",
    "        K =\n",
    "        V =\n",
    "        \n",
    "        # Q x K^T \n",
    "        dot =\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask to attention score\n",
    "            # Set the \n",
    "            attention_score =\n",
    "            mask_value = torch.finfo(dots.dtype).min\n",
    "            mask = # you have transform the mask maybe\n",
    "            dot.masked_fill_(~mask, mask_value) # If you set the mask value to -inf after the softmax you will get 0\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_score =\n",
    "        \n",
    "        # Scale values by sqrt(dim_head)\n",
    "        attention_score /= \n",
    "        \n",
    "        # Compute attention score times V\n",
    "        head =\n",
    "        \n",
    "        return head, attention_score\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(input_dim=64, dim_head=16)\n",
    "head, attention_score = sa(dummy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 2.0: </b>\n",
    "* Implement the multihead attention mechanism\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5  # 1/sqrt(dim)\n",
    "        \"\"\"To qkv is an optimisation, if you are brave you can try to implement it\"\"\"\n",
    "        # self.to_qkv = nn.Linear(\n",
    "        #     dim, inner_dim * 3, bias=False\n",
    "        # )  # Wq,Wk,Wv for each vector, thats why *3\n",
    "        self.query = nn.Linear(dim, inner_dim)\n",
    "        self.key = nn.Linear(dim, inner_dim)\n",
    "        self.value = nn.Linear(dim, inner_dim)\n",
    "        self.to_out = nn.Linear(inner_dim, inner_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) :\n",
    "        h = self.heads\n",
    "\n",
    "        # # gets q = Q = Wq matmul x1, k = Wk mm x2, v = Wv mm x3\n",
    "        # qkv = self.to_qkv(x)\n",
    "        \n",
    "        Q = \n",
    "        K =\n",
    "        V = \n",
    "        \n",
    "        ...\n",
    "        \n",
    "        return out, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(64, 8, 64)\n",
    "out, attention_scores = mha(dummy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 3.0: </b>\n",
    "* Implement the tranformer block encoder block\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mha = MultiHeadAttention(dim, heads, dim_head)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask) # The addition is a skip connextion do not hesitate to raise your hand for more details ;)\n",
    "        ...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teb = TransformerEncoderBlock(64, 8, 64)\n",
    "out = teb(dummy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> (Optional) Exercise 4.0: </b>\n",
    "* Implement the tranformer block decoder block\n",
    "* Combine the encoder and decoder to create a transformer\n",
    "* Implement the positional encoding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranformerDecoder(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LLM is just a bunch of transformers encoder and decoder layer with large feedforward dimension and a lot of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the purpose of Bert? 🤖\n",
    "\n",
    "BERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream tasks.\n",
    "\n",
    "### What is a downstream task? 🧐\n",
    "\n",
    "A downstream task is when a pre-trained model is used for a new task. For example, you could train Bert on the question answering task. You would benefit from Bert's pre-trained weights for this new task.\n",
    "\n",
    "### Why Bert like models outperform other models? 🎯\n",
    "\n",
    "BERT outperforms previous methods because it is the first **unsupervised**, deeply bidirectional system for pre-training NLP. \n",
    "\n",
    "`I made a bank deposit` \n",
    "\n",
    "In the example, the unidirectional representation of the token `bank` is only based on `I made` but not on `deposit`.\n",
    "\n",
    "BERT represents `bank` using both its left and right context starting from the very bottom of a deep neural network, so it is deeply bidirectional.\n",
    "\n",
    "The ELMo and OpenAI GPT models are other high-performance models that provide contextual latent representations. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. OpenAI GPT uses a left-to-right Transformer. \n",
    "\n",
    "### How Bert is trained? 🔧\n",
    "\n",
    "BERT is trained from **two unsupervised tasks**.\n",
    "\n",
    "#### Task 1: Masked LM\n",
    "\n",
    "**15%** of the words in the input sentence are masked. The model must then find the words that have been hidden.\n",
    "\n",
    "```python\n",
    "input = 'the man went to the [MASK1]. he bought a [MASK2] of milk'\n",
    "label = {'[MASK1]': 'store', '[MASK2]': 'gallon'}`\n",
    "```\n",
    "\n",
    "\n",
    "Tips used in the Masked LM task 😎: \n",
    "\n",
    "- 80% of the time: Replace the word with the `[MASK]` token, e.g., `my dog is hair` → `my dog is [MASK]`\n",
    "\n",
    "\n",
    "- 10% of the time: Replace the word with a randomword,e.g., my `dog is hair` → `my dog is apple`\n",
    "\n",
    "\n",
    "- 10% of the time: Keep the word unchanged,e.g., `my dog is hair` → `my dog is hair`. \n",
    "\n",
    "The model is forced to keep a distributional contextual representation of every input token because the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words.\n",
    "\n",
    "\n",
    "#### Task 2: Next sentence prediction\n",
    "\n",
    "This task is designed to provide BERT an understanding of the relationship between two sentences. This information is not captured by the masked language model task. When choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n",
    "\n",
    "```python\n",
    "\n",
    "input = '[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]'\n",
    "label = 'IsNext'\n",
    "\n",
    "input = '[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]'\n",
    "label = 'NotNext'\n",
    "\n",
    "```\n",
    "\n",
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "- **[SEP]**: Separator between sentences.\n",
    "- **[CLS]**: Token dedicated to classification tasks.\n",
    "\n",
    "**The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.**\n",
    "\n",
    "\n",
    "### What are the corpus used to train BERT?\n",
    "\n",
    "- BooksCorpus (800M words) 📖\n",
    "\n",
    "- English Wikipedia (2,500M words) 📚\n",
    "\n",
    "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus in order to extract long contiguous sequences.\n",
    "\n",
    "### What is Finetuning ?\n",
    "\n",
    "Finetuning consists of training the BERT model on a new task to specialize the model. BERT finetuning is relatively inexpensive. It is necessary to add additional layers of neurons to transform the model into a classifier or regressor for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between BERT and GPT?\n",
    "\n",
    "GPT model family are based on the Transformer decoder. They are trained using a causal language modeling (CLM) objective. The model learns to predict the next token in a sequence given the previous tokens.\n",
    "\n",
    "### What dataset is used to train GPT?\n",
    "For the most recent version of GPT (3&4), the dataset used is the entire public internet. With the combination of a huge dataset, an self-supervised objective and a large model, GPT-3&4 can perform well on a wide variety of tasks.\n",
    "\n",
    "### The last secret sauce of GPT 3&4: RLHF\n",
    "Why chatgpt feel like a discussion with an human, why chat gpt is so good at generating text? The answer is RLHF (Reinforcement Learning with Human Feedback). The model learn using reinforcement to generate an answer which is the most aligned with human. If you are curious about it, there is a project on reinforcement learning with human feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-tuning BERT for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is a function that splits a string of text into tokens. For example, the string \"I love Paris\" could be tokenized into the list of tokens [\"I\", \"love\", \"Paris\"]. There are different types of tokenizers, such as word tokenizers that split strings into words, character tokenizers that split strings into characters, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 5.0: </b>\n",
    "* Explain with your own words why we need to tokenize the input\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\n",
    "    \"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 6.0: </b>\n",
    "* Find in the transformers library tes arguments for : download the bert uncased model \"bert-base-cased\" and change the number of labels to 5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 7.0: </b>\n",
    "* make an inference with the model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Extract the first example\n",
    "example = small_train_dataset[0]\n",
    "\n",
    "# Extract the input fields required by the model\n",
    "input_ids = ...\n",
    "attention_mask = ... \n",
    "\n",
    "# Convert to the format expected by the model (as tensors)\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "}\n",
    "\n",
    "\n",
    "# Get predictions from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The outputs are logits, you can apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    nb_iteration = 0\n",
    "    total_data = 0\n",
    "    for batch in dataloader:\n",
    "        # Move batch to the same device as model\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        y = batch.pop(\"label\").to(device)\n",
    "        text = batch.pop(\"text\")\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input_data = {k: batch[k] for k in (\"input_ids\", \"attention_mask\")}\n",
    "        input_data = {k: v.to(device) for k, v in input_data.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_data)\n",
    "        logits = outputs.logits\n",
    "        total_data += logits.size()[0]  # Get the batch size\n",
    "        loss = loss_function(logits, y)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == y)\n",
    "        nb_iteration += 1\n",
    "\n",
    "    avg_loss = total_eval_loss / nb_iteration\n",
    "    accuracy = correct_predictions.double() / total_data\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 8.0: </b>\n",
    "* Add the AdamW optimizer with an LR of 5e-5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import  ...\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Ensure the model is on the correct device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = ... \n",
    "\n",
    "# Define the loss function\n",
    "loss_function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 9.0: </b>\n",
    "* Perform the training\n",
    "* Why the performance is so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for displaying a progress bar\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    total_loss = 0\n",
    "    nb_iteration = 0\n",
    "\n",
    "    train_iter = small_train_dataset.iter(batch_size=8)\n",
    "    eval_iter = small_eval_dataset.iter(batch_size=8)\n",
    "\n",
    "    for batch in tqdm(train_iter, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
    "        # Move batch to the same device as model\n",
    "        y = batch.pop(\"label\").to(device)\n",
    "        text = batch.pop(\"text\")\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input_data = {k: batch[k] for k in (\"input_ids\", \"attention_mask\")}\n",
    "        input_data = {k: v.to(device) for k, v in input_data.items()}\n",
    "        outputs = model(**input_data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs.logits, y)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters and zero the gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        nb_iteration += 1\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss over an epoch\n",
    "    avg_train_loss = total_loss / nb_iteration\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} complete! Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    eval_loss, eval_accuracy = evaluate_model(model, eval_iter, loss_function, device)\n",
    "    print(f\"Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 10.0: </b>\n",
    "* Reach the best performance possible, add learning rate scheduler, increase the training dataset size etc ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
