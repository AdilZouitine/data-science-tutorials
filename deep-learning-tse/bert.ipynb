{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet another practical introduction to Bert\n",
    "\n",
    "### Rapha√´l Sourty\n",
    "\n",
    "#### Toulouse School of Economics Master's degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the purpose of Bert? ü§ñ\n",
    "\n",
    "BERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream tasks.\n",
    "\n",
    "### What is a downstream task? üßê\n",
    "\n",
    "A downstream task is when a pre-trained model is used for a new task. For example, you could train Bert on the question answering task. You would benefit from Bert's pre-trained weights for this new task.\n",
    "\n",
    "### Why Bert outperform other models? üéØ\n",
    "\n",
    "BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP. \n",
    "\n",
    "`I made a bank deposit` \n",
    "\n",
    "In the example, the unidirectional representation of the token `bank` is only based on `I made` but not on `deposit`.\n",
    "\n",
    "BERT represents `bank` using both its left and right context starting from the very bottom of a deep neural network, so it is deeply bidirectional.\n",
    "\n",
    "The ELMo and OpenAI GPT models are other high-performance models that provide contextual latent representations. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. OpenAI GPT uses a left-to-right Transformer. \n",
    "\n",
    "### How Bert is trained? üîß\n",
    "\n",
    "BERT is trained from **two unsupervised tasks**.\n",
    "\n",
    "#### Task 1: Masked LM\n",
    "\n",
    "**15%** of the words in the input sentence are masked. The model must then find the words that have been hidden.\n",
    "\n",
    "```python\n",
    "input = 'the man went to the [MASK1]. he bought a [MASK2] of milk'\n",
    "label = {'[MASK1]': 'store', '[MASK2]': 'gallon'}`\n",
    "```\n",
    "\n",
    "\n",
    "Tips used in the Masked LM task üòé: \n",
    "\n",
    "- 80% of the time: Replace the word with the `[MASK]` token, e.g., `my dog is hair` ‚Üí `my dog is [MASK]`\n",
    "\n",
    "\n",
    "- 10% of the time: Replace the word with a randomword,e.g., my `dog is hair` ‚Üí `my dog is apple`\n",
    "\n",
    "\n",
    "- 10% of the time: Keep the word unchanged,e.g., `my dog is hair` ‚Üí `my dog is hair`. \n",
    "\n",
    "The model is forced to keep a distributional contextual representation of every input token because the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words.\n",
    "\n",
    "\n",
    "#### Task 2: Next sentence prediction\n",
    "\n",
    "This task is designed to provide BERT an understanding of the relationship between two sentences. This information is not captured by the masked language model task. When choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n",
    "\n",
    "```python\n",
    "\n",
    "input = '[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]'\n",
    "label = 'IsNext'\n",
    "\n",
    "input = '[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]'\n",
    "label = 'NotNext'\n",
    "\n",
    "```\n",
    "\n",
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "- **[SEP]**: Separator between sentences.\n",
    "- **[CLS]**: Token dedicated to classification tasks.\n",
    "\n",
    "**The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.**\n",
    "\n",
    "\n",
    "### What are the corpus used to train BERT?\n",
    "\n",
    "- BooksCorpus (800M words) üìñ\n",
    "\n",
    "- English Wikipedia (2,500M words) üìö\n",
    "\n",
    "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus in order to extract long contiguous sequences.\n",
    "\n",
    "### What is Finetuning ?\n",
    "\n",
    "Finetuning consists of training the BERT model on a new task to specialize the model. BERT finetuning is relatively inexpensive. It is necessary to add additional layers of neurons to transform the model into a classifier or regressor for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pre-process text to feed BERT?\n",
    "\n",
    "BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.\n",
    "\n",
    "### How does BERT handle OOV words?\n",
    "\n",
    "Any word that does not occur in the vocabulary is broken down into sub-words greedily. \n",
    "\n",
    "For example, if `play`, `##ing`, and `##ed` are present in the vocabulary but `playing` and `played` are OOV words then they will be broken down into `play` + `##ing` and `play` + `##ed` respectively.`\n",
    "\n",
    "`##` is used to represent sub-words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of BERT parameters:\n",
    "\n",
    "\n",
    "#### BERTBASE: \n",
    "\n",
    "- Number of transformer blocks: 12, \n",
    "- Hidden size: 768, \n",
    "- Number of self-attention heads: 12\n",
    "- Total Parameters: 110M \n",
    "\n",
    "#### ü§Ø BERTLARGE: \n",
    "\n",
    "- Number of transformer blocks: 24\n",
    "- Hidden size: 1024\n",
    "- Number of self-attention heads: 16\n",
    "- Total Parameters: 340M \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"curve_parameters_models.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the language models depends on, among other things:\n",
    "    \n",
    "    - the quality of the training data\n",
    "    \n",
    "    - the volume of training data\n",
    "    \n",
    "    - the quality of the model architecture\n",
    "    \n",
    "    - the number of model parameters\n",
    "    \n",
    "   \n",
    "There are some interesting works that allow to compress the knowledge of language models. The objective of this area of research is to build models with fewer (lighter) parameters while avoiding reducing their accuracy. **DistillBERT** is one of these models. **DistillBERT** tends to reproduce BERT's results with far fewer parameters than its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a method dedicated to compress knowledge. It consists in transmitting knowledge from a teacher to a student. üë©‚Äçüè´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks typically produce class probabilities by using a ‚Äúsoftmax‚Äù output layer that converts the logit, $zi$, computed for each class into a probability, $qi$, by comparing $zi$ with the other logits. `Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{i}=\\frac{\\exp \\left(z_{i} / T\\right)}{\\sum_{j} \\exp \\left(z_{j} / T\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_s = (1 - \\alpha) * KL(q^{t}_{i}, q^{s}_{i}) + \\alpha * \\mathcal{H}_{s}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> \n",
    "    $\\mathcal{H}_{s}$ is the loss of the student such as cross-entropy loss function.\n",
    "</p> \n",
    "\n",
    "<p style=\"text-align: center;\"> \n",
    "    $q^{t}$ is the probability distribution in output of teacher.\n",
    "</p> \n",
    " \n",
    "<p style=\"text-align: center;\"> \n",
    "    $q^{s}$ is the probability distribution in output of the student.\n",
    "</p> \n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"> \n",
    "    $KL$ denotes the Kullback-Leibler divergence, $KL(P \\| Q)=\\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}$  \n",
    "</p> \n",
    "\n",
    "Tips:\n",
    "\n",
    "- By making the coefficient $\\alpha$ evolve linearly as the training progresses, the student can overtake the teacher.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HuggingFace organization used knowledge distillation to concentrate BERT's knowledge into a much smaller model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concretely, what are the results obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{lcc}\n",
    "\\hline \\text { Model } & \\begin{array}{c}\n",
    "\\text { IMDb } \\\\\n",
    "\\text { (acc.) }\n",
    "\\end{array} & \\begin{array}{c}\n",
    "\\text { SQuAD } \\\\\n",
    "\\text { (EM/F1) }\n",
    "\\end{array} \\\\\n",
    "\\hline \\text { BERT-base } & 93.46 & 81.2 / 88.5 \\\\\n",
    "\\text { DistilBERT } & 92.82 & 77.7 / 85.8 \\\\\n",
    "\\text { DistilBERT (D) } & - & 79.1 / 86.9 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcc}\n",
    "\\hline \\text { Model } & \\begin{array}{c}\n",
    "\\# \\text { param. } \\\\\n",
    "\\text { (Millions) }\n",
    "\\end{array} & \\begin{array}{c}\n",
    "\\text { Inf. time } \\\\\n",
    "\\text { (seconds) }\n",
    "\\end{array} \\\\\n",
    "\\hline \\text { ELMo } & 180 & 895 \\\\\n",
    "\\text { BERT-base } & 110 & 668 \\\\\n",
    "\\text { DistilBERT } & 66 & 410 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "DistilBERT yields to comparable performance on downstream tasks. \n",
    "\n",
    "**IMDb**: Binary sentiment classification dataset.\n",
    "\n",
    "**SQuAD**: The Stanford Question Answering Dataset.\n",
    "\n",
    "`Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification Challenge\n",
    "\n",
    "#### Identify and classify toxic online comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertModel\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import metrics\n",
    "from creme import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import sklearn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('./jigsaw-toxic-comment-classification-challenge.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(zf.open('train.csv.zip'), compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = pd.read_csv(zf.open('test.csv.zip'), compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(zf.open('sample_submission.csv.zip'), compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(zf.open('test_labels.csv.zip'), compression = 'zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "X_test['comment_text'] = X_test['comment_text'].map(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    train, \n",
    "    train[labels], \n",
    "    test_size=0.20, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype = torch.float)\n",
    "y_valid = torch.tensor(y_valid.values, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, tokenizer, max_len, y):\n",
    "        self.len = len(X)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        comment_text = str(self.X['comment_text'][index])\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            text_pair             = None,\n",
    "            add_special_tokens    = True,\n",
    "            truncation            = True,\n",
    "            max_length            = self.max_len,\n",
    "            padding               = 'max_length',\n",
    "            return_token_type_ids = True,\n",
    "        )\n",
    "        \n",
    "        ids  = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'index': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask' : torch.tensor(mask, dtype=torch.long),\n",
    "            'y'    : self.y[index]\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "max_len   = tokenizer.max_model_input_sizes['distilbert-base-uncased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len,\n",
    ")\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    X = X_valid, \n",
    "    y = y_valid,\n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len,\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    X = X_test, \n",
    "    y = torch.tensor(test_labels[labels].values),\n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "params = {\n",
    "    'batch_size' : batch_size,\n",
    "    'shuffle'    : True,\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "params_test = {\n",
    "    'batch_size' : batch_size,\n",
    "    'shuffle'    : False,\n",
    "    'num_workers': 0,\n",
    "}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    **params\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    **params_test\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    **params_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBert(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DistillBert, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.l2(pooler)\n",
    "        pooler = self.l3(pooler)\n",
    "        return pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistillBert()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params = model.parameters(), \n",
    "    lr     = 1e-04,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingF1:\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.score = stats.RollingMean(k)\n",
    "        \n",
    "    def update(self, y_true, y_pred):\n",
    "\n",
    "        y_true = y_true.cpu().detach()\n",
    "        y_pred = y_pred.cpu().detach()\n",
    "        \n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        \n",
    "        y_pred = y_pred.numpy()\n",
    "        \n",
    "        for i, pred in enumerate(y_pred):\n",
    "\n",
    "            self.score.update(\n",
    "                sklearn.metrics.f1_score(y_true = y_true[i], y_pred = pred, zero_division=1)\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get(self):\n",
    "        return self.score.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    bar = tqdm.tqdm(enumerate(train_loader), position = 0, total = len(train_loader))\n",
    "\n",
    "    metric_loss = stats.RollingMean(len(train_loader))\n",
    "\n",
    "    metric_f1 = RollingF1(len(train_loader))\n",
    "\n",
    "    for step, data in bar:\n",
    "        \n",
    "        index = data['index'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "        y     = data['y'].to(device)\n",
    "        \n",
    "        y_pred = model(index, mask)\n",
    "        \n",
    "        loss = loss_function(y_pred, target = y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            metric_f1.update(y_pred = y_pred, y_true = y)\n",
    "            \n",
    "        metric_loss.update(loss.item())\n",
    "        \n",
    "        if step % 3 == 0:\n",
    "        \n",
    "            bar.set_description(\n",
    "                f'Epoch: {epoch + 1}, Loss: {metric_loss.get():6f}, F1: {metric_f1.get():6f}'\n",
    "            )\n",
    "    break\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        metric_f1 = RollingAUC(len(test_loader))\n",
    "        \n",
    "        for data in valid_loader:\n",
    "\n",
    "            index = data['index'].to(device)\n",
    "            mask  = data['mask'].to(device)\n",
    "            y     = data['y'].to(device)\n",
    "\n",
    "            y_pred = model(index, mask)\n",
    "\n",
    "            metric_f1.update(y_pred = y_pred, y_true = y)\n",
    "\n",
    "        print(f'\\n Epoch: {epoch + 1}, Valid - F1: {metric_f1.get():6f}\\n')\n",
    "        \n",
    "    model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "bar = tqdm.tqdm(test_loader, position = 0, total = len(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for data in bar:\n",
    "\n",
    "        index = data['index'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "\n",
    "        y_pred.append(model(index, mask))\n",
    "\n",
    "y_pred = pd.DataFrame(torch.cat(y_pred).numpy())\n",
    "y_pred.columns = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([sample_submission['id'], y_pred], axis = 'columns', sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./../data/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the energy consumption of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"energy.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "**To give us an idea...** ‚úàÔ∏è\n",
    "\n",
    "**100lbs ~= 45 kgs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bert_energy.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "\n",
    "NAS: neural architecture search for machine translation and language modeling. NAS base model requires 10 hours to train for 300k steps on one TPUv2 core. This equates to 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs.\n",
    "\n",
    "`Strubell, Emma, Ananya Ganesh, and Andrew McCallum. \"Energy and policy considerations for deep learning in NLP.\" arXiv preprint arXiv:1906.02243 (2019).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIPS:\n",
    "\n",
    "- How to speed up bert in production?\n",
    "\n",
    "- How to effectively fine tune bert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "---\n",
    "\n",
    "Understanding LSTM Networks\n",
    "\n",
    "url: https://colah.github.io/posts/2015-08-Understanding-LSTMs/?fbclid=IwAR27M5XcLHjege9JDK94xMPAmbUYZTf9BXCYQSoGTzfI00P1PGa3U0mW6rY\n",
    "\n",
    "---\n",
    "Publication introducing Bert.\n",
    "\n",
    "title: \"Bert: Pre-training of deep bidirectional transformers for language understanding\"\n",
    "\n",
    "author: Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina\n",
    "\n",
    "year: 2018\n",
    "\n",
    "url: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "---\n",
    "\n",
    "The Illustrated Transformer.\n",
    "\n",
    "url: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "---\n",
    "\n",
    "Transformers from scratch.\n",
    "\n",
    "http://peterbloem.nl/blog/transformers\n",
    "\n",
    "---\n",
    "\n",
    "Huggingface, transformers library:\n",
    "\n",
    "url: https://github.com/huggingface/transformers\n",
    "\n",
    "---\n",
    "\n",
    "Source code of Bert.\n",
    "\n",
    "url: https://github.com/google-research/bert\n",
    "\n",
    "---\n",
    "\n",
    "Bert explained.\n",
    "\n",
    "url: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "---\n",
    "\n",
    "Energy consumption of deep neural network models:\n",
    "\n",
    "title: \"Energy and policy considerations for deep learning in NLP.\"\n",
    "\n",
    "author: Strubell, Emma, Ananya Ganesh, and Andrew McCallum.\n",
    "\n",
    "year: 2019\n",
    "\n",
    "url: https://arxiv.org/abs/1906.02243\n",
    "\n",
    "---\n",
    "\n",
    "BERT Explained ‚Äì A list of Frequently Asked Questions\n",
    "\n",
    "url: https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/\n",
    "\n",
    "---\n",
    "\n",
    "Wordpiece tokenizer:\n",
    "\n",
    "title: \"Japanese and korean voice search.\"\n",
    "\n",
    "author: Schuster, Mike, and Kaisuke Nakajima.\n",
    "\n",
    "year: 2012 \n",
    "\n",
    "url: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf\n",
    "\n",
    "---\n",
    "\n",
    "title: \"Distilling the knowledge in a neural network.\"\n",
    "\n",
    "author: Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \n",
    "\n",
    "year: 2014\n",
    "\n",
    "url: https://arxiv.org/abs/1503.02531\n",
    "\n",
    "---\n",
    "\n",
    "title: \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\"\n",
    "\n",
    "author: Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas\n",
    "\n",
    "year: 2019\n",
    "\n",
    "url: https://arxiv.org/abs/1910.01108\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
